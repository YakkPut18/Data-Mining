# -*- coding: utf-8 -*-
"""Smartpeak.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OlnQPLQtawnnedpUjlatNL9E2LIaqtfm
"""

!pip install apify_client pandas

"""# SENTIMEN SEC"""

from apify_client import ApifyClient

# Initialize the ApifyClient with your API token
client = ApifyClient("apify_api_eMT9mGEyqurwyZjLadIFX9g0juNber3xwyE1")

# Prepare the Actor input
run_input = {
    "commentsPerPost": 300,
    "excludePinnedPosts": False,
    "maxRepliesPerComment": 5,
    "postURLs": [
        "https://www.tiktok.com/@azkhayeyeye/video/7520133823577820422?is_from_webapp=1&sender_device=pc&web_id=7547567738635879954",
        "https://www.tiktok.com/@ahlmsrya/video/7377730197807828230?is_from_webapp=1&sender_device=pc&web_id=7547567738635879954",
        "https://www.tiktok.com/@masnganggur62/video/7375825373755411718?is_from_webapp=1&sender_device=pc&web_id=7547567738635879954",
        "https://www.tiktok.com/@adrianhusaeni_10/video/7535033412135013637?is_from_webapp=1&sender_device=pc&web_id=7547567738635879954",
        "https://www.tiktok.com/@aldzanandra/video/7500205969029287223?is_from_webapp=1&sender_device=pc&web_id=7547567738635879954"
    ],
    "resultsPerPage": 100
}

# Run the Actor and wait for it to finish
run = client.actor("BDec00yAmCm1QbMEI").call(run_input=run_input)

items = list(client.dataset(run["defaultDatasetId"]).iterate_items())

import pandas as pd
df = pd.DataFrame(items)
df

df.to_excel("Smartpeak.xlsx",index=False)

"""Filter Data"""

import pandas as pd
import re

# Load data dari file Excel
df = pd.read_excel('Smartpeak.xlsx')

def clean_tiktok_text(text):
    """
    Fungsi pembersihan teks khusus untuk komentar TikTok Indonesia
    """
    if pd.isna(text) or not isinstance(text, str):
        return ""

    # 1. Convert ke lowercase
    text = text.lower()

    # 2. Hapus URL/link
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # 3. Hapus mention (@username)
    text = re.sub(r'@\w+', '', text)

    # 4. Hapus hashtag (#) tapi pertahankan teksnya
    text = re.sub(r'#(\w+)', r'\1', text)

    # 5. Hapus karakter HTML entities
    text = re.sub(r'&\w+;', '', text)

    # 6. Pertahankan emoji untuk analisis sentimen (opsional)
    # Jika ingin hapus emoji, uncomment baris berikut:
    # text = re.sub(r'[^\w\s]', ' ', text)

    # 7. Hapus tanda baca berlebihan tapi pertahankan titik dan koma
    text = re.sub(r'[^\w\s.,!?-]', ' ', text)

    # 8. Hapus angka yang berdiri sendiri (opsional)
    text = re.sub(r'\b\d+\b', '', text)

    # 9. Normalisasi spasi
    text = re.sub(r'\s+', ' ', text)

    # 10. Strip whitespace
    text = text.strip()

    return text

def advanced_clean_indonesian(text):
    """
    Pembersihan lanjutan khusus bahasa Indonesia informal
    """
    if not text:
        return ""

    # Normalisasi kata-kata slang/informal Indonesia
    slang_dict = {
        'gw': 'saya',
        'gue': 'saya',
        'lu': 'kamu',
        'lo': 'kamu',
        'emg': 'memang',
        'udh': 'sudah',
        'udah': 'sudah',
        'krn': 'karena',
        'karna': 'karena',
        'tp': 'tapi',
        'sm': 'sama',
        'dr': 'dari',
        'dg': 'dengan',
        'dgn': 'dengan',
        'jg': 'juga',
        'jgn': 'jangan',
        'ga': 'tidak',
        'gk': 'tidak',
        'gak': 'tidak',
        'yg': 'yang',
        'bgt': 'banget',
        'bgt': 'banget',
        'klo': 'kalau',
        'kalo': 'kalau'
    }

    # Apply slang normalization
    words = text.split()
    normalized_words = [slang_dict.get(word, word) for word in words]
    text = ' '.join(normalized_words)

    return text

def filter_relevant_content(text):
    """
    Filter konten yang relevan dengan FOMO pendakian
    """
    if not text:
        return False

    # Kata kunci FOMO pendakian
    fomo_keywords = [
        'fomo', 'ikut tren', 'viral', 'trending', 'hits', 'ngehits',
        'ketinggalan', 'pengen ikutan', 'biar keren'
    ]

    hiking_keywords = [
        'gunung', 'mendaki', 'pendakian', 'hiking', 'summit', 'puncak',
        'prau', 'rinjani', 'semeru', 'merapi', 'bromo', 'pendaki',
        'naik gunung', 'basecamp', 'jalur', 'trek', 'mdpl'
    ]

    risk_keywords = [
        'bahaya', 'risiko', 'kecelakaan', 'sesat', 'tersesat',
        'hipotermia', 'kedinginan', 'cuaca buruk', 'kabut',
        'tidak siap', 'modal nekat', 'tanpa persiapan'
    ]

    text_lower = text.lower()

    # Check relevance
    has_fomo = any(keyword in text_lower for keyword in fomo_keywords)
    has_hiking = any(keyword in text_lower for keyword in hiking_keywords)
    has_risk = any(keyword in text_lower for keyword in risk_keywords)

    return has_fomo or has_hiking or has_risk

# Terapkan pembersihan teks
print("Memulai pembersihan teks...")

# Step 1: Basic cleaning
df['text_clean'] = df['text'].apply(clean_tiktok_text)

# Step 2: Advanced Indonesian normalization
df['text_normalized'] = df['text_clean'].apply(advanced_clean_indonesian)

# Step 3: Filter konten relevan
df['is_relevant'] = df['text_normalized'].apply(filter_relevant_content)

# Step 4: Hapus teks yang terlalu pendek atau kosong
df = df[df['text_normalized'].str.len() >= 10]  # Minimal 10 karakter
df = df[df['is_relevant'] == True]  # Hanya konten relevan

# Tampilkan statistik
print(f"Total data setelah pembersihan: {len(df)}")
print(f"Data relevan: {df['is_relevant'].sum()}")

# Contoh hasil pembersihan
print("\n=== CONTOH HASIL PEMBERSIHAN ===")
for i in range(5):
    if i < len(df):
        print(f"\nOriginal: {df.iloc[i]['text']}")
        print(f"Cleaned:  {df.iloc[i]['text_normalized']}")

# Simpan hasil ke file CSV untuk analisis lanjutan
df_clean = df[['text', 'text_clean', 'text_normalized', 'is_relevant', 'createTime']].copy()
df_clean.to_csv('tiktok_comments_cleaned.csv', index=False, encoding='utf-8')

print(f"\nData bersih disimpan ke: 'tiktok_comments_cleaned.csv'")
print(f"Siap untuk analisis sentimen dengan IndoBERTweet!")

def create_indonesian_sentiment_keywords():
    """
    Keyword sentimen khusus untuk konteks media sosial Indonesia
    """

    sentiment_dict = {
        'negative': [
            # Kata negatif umum
            'buruk', 'jelek', 'tidak bagus', 'mengecewakan', 'gagal',
            'rugi', 'bohong', 'penipuan', 'tipu', 'scam', 'hoax',

            # Emosi negatif
            'marah', 'benci', 'sedih', 'takut', 'cemas', 'khawatir',
            'stress', 'depresi', 'frustrasi', 'kesal', 'jengkel',

            # Bahaya dan risiko
            'bahaya', 'berbahaya', 'risiko', 'ancaman', 'merugikan',
            'toxic', 'jahat', 'kejam', 'brutal',

            # FOMO dan tekanan sosial
            'fomo', 'takut ketinggalan', 'tertekan', 'minder', 'insecure',

            # Slang negatif Indonesia
            'zonk', 'jelek banget', 'parah', 'amit amit', 'ngeri',
            'serem', 'mengerikan', 'menyebalkan', 'bikin kesel','manfaat mendaki','mending tidur'
        ],

        'positive': [
            # Kata positif umum
            'bagus', 'baik', 'hebat', 'luar biasa', 'amazing', 'keren',
            'mantap', 'top', 'terbaik', 'sempurna', 'excellent', 'terima kasih'

            # Emosi positif
            'senang', 'gembira', 'bahagia', 'puas', 'bangga', 'excited',
            'antusias', 'optimis', 'percaya diri', 'tenang',

            # Rekomendasi dan approval
            'recommended', 'rekomendasi', 'worth it', 'pantas', 'layak',
            'cocok', 'pas', 'tepat', 'sesuai', 'oke banget','asik',

            # Slang positif Indonesia
            'mantul', 'mantap jiwa', 'kece', 'juara', 'jos gandos',
            'pecah', 'lit', 'fire', 'dope', 'sick', 'gokil positif','sehat','pingin','mau','kecanduan','gas'
        ],

        'neutral': [
            # Kata netral umum
            'biasa', 'standar', 'normal', 'wajar', 'lumayan', 'cukup',
            'oke', 'tidak apa', 'gpp', 'fine', 'so-so',

            # Ekspresi netral
            'mungkin', 'sepertinya', 'kayaknya', 'agak', 'sedikit',
            'cukup', 'relatif', 'tergantung', 'bisa jadi',

            # Objektif
            'fakta', 'data', 'informasi', 'berita', 'update',
            'laporan', 'analisis', 'studi', 'penelitian', 'biarin'
        ]
    }

    return sentiment_dict

def advanced_sentiment_classification(df, column='text_clean'):
    """
    Klasifikasi sentimen lanjutan dengan konteks Indonesia
    """

    sentiment_dict = create_indonesian_sentiment_keywords()

    def classify_advanced_sentiment(text):
        text_lower = text.lower()
        scores = {}
        matched_words = {}

        # Hitung score dan track kata yang match
        for sentiment, keywords in sentiment_dict.items():
            score = 0
            matches = []

            for keyword in keywords:
                if keyword in text_lower:
                    score += 1
                    matches.append(keyword)

            scores[sentiment] = score
            matched_words[sentiment] = matches

        # Tentukan sentimen dominan
        max_sentiment = max(scores, key=scores.get)
        max_score = scores[max_sentiment]

        # Jika tidak ada kata yang match atau semua sama
        if max_score == 0 or list(scores.values()).count(max_score) > 1:
            final_sentiment = 'neutral'
            confidence = 0.3
        else:
            final_sentiment = max_sentiment
            total_matches = sum(scores.values())
            confidence = max_score / max(total_matches, 1)

        return {
            'sentiment': final_sentiment,
            'confidence': confidence,
            'scores': scores,
            'matched_words': matched_words
        }

    # Apply classification
    results = df[column].apply(classify_advanced_sentiment)

    # Extract to columns
    df['predicted_sentiment'] = [r['sentiment'] for r in results]
    df['confidence'] = [r['confidence'] for r in results]
    df['sentiment_scores'] = [r['scores'] for r in results]
    df['matched_words'] = [r['matched_words'] for r in results]

    return df

# Implementasi klasifikasi lanjutan
df_advanced = advanced_sentiment_classification(df)

def complete_sentiment_pipeline(df, approaches=['advanced']):
    """
    Pipeline lengkap dengan multiple approaches, only using defined functions
    """

    results = {}

    if 'advanced' in approaches:
        print("Running advanced sentiment classification...")
        df_advanced = advanced_sentiment_classification(df.copy())
        results['advanced'] = df_advanced
    else:
        print("No valid sentiment analysis approach selected.")
        return None


    # Display results for the selected approach
    print("\n=== RESULTS ===")
    for approach, data in results.items():
        sentiment_col = 'predicted_sentiment'
        if sentiment_col in data.columns:
            dist = data[sentiment_col].value_counts()
            print(f"\n{approach.upper()} Sentiment Distribution:")
            print(dist)

    return results

# Jalankan pipeline lengkap
all_results = complete_sentiment_pipeline(df)

# Filter based on the result (assuming 'advanced' is the only approach)
if all_results and 'advanced' in all_results:
    best_result = all_results['advanced']

    # Separate by sentiment
    positive_tweets = best_result[best_result['predicted_sentiment'] == 'positive']
    negative_tweets = best_result[best_result['predicted_sentiment'] == 'negative']
    neutral_tweets = best_result[best_result['predicted_sentiment'] == 'neutral']

    # Export results
    positive_tweets.to_csv('tweets_positive_filtered.csv', index=False)
    negative_tweets.to_csv('tweets_negative_filtered.csv', index=False)
    neutral_tweets.to_csv('tweets_neutral_filtered.csv', index=False)

    print(f"\nFinal Results:")
    print(f"Positive: {len(positive_tweets)} tweets")
    print(f"Negative: {len(negative_tweets)} tweets")
    print(f"Neutral: {len(neutral_tweets)} tweets")
else:
    print("Pipeline did not produce results.")

def validate_sentiment_classification(df, sample_size=10):
    """
    Validasi hasil klasifikasi sentimen
    """

    print("=== VALIDASI KLASIFIKASI SENTIMEN ===\n")

    sentiment_types = df['predicted_sentiment'].unique()

    for sentiment in sentiment_types:
        print(f"\n{sentiment.upper()} SAMPLES:")
        print("-" * 50)

        sentiment_data = df[df['predicted_sentiment'] == sentiment]
        sample = sentiment_data.head(sample_size)

        for idx, row in sample.iterrows():
            print(f"{idx+1}. {row['text_normalized']}")
            if 'confidence' in row:
                print(f"   Confidence: {row['confidence']:.2f}")
            if 'matched_words' in row:
                print(f"   Matched words: {row['matched_words']}")
            print()

# Jalankan validasi
validate_sentiment_classification(best_result, sample_size=5)

"""# Set Up IndoBERTweet

Installing Packages
"""

# Install required packages
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install transformers
!pip install datasets
!pip install accelerate
!pip install sentencepiece

# Import required libraries
import torch
import pandas as pd
import numpy as np
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

"""SETUP INDOBERTWEET MODEL"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import pandas as pd
import numpy as np
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class IndoBERTweetSentimentAnalyzer:
    """
    Class untuk IndoBERTweet Sentiment Analysis menggunakan model yang tersedia
    """

    def __init__(self, model_choice='aardiiiiy'):
        """
        Initialize dengan model pilihan yang tersedia
        """

        # Daftar model IndoBERTweet yang tersedia untuk sentiment analysis
        available_models = {
            'aardiiiiy': 'Aardiiiiy/indobertweet-base-Indonesian-sentiment-analysis',
            'ridhodaffasyah': 'ridhodaffasyah/sentiment-analysis-indobertweet',
            'rikidharmawan': 'rikidharmawan/finetuning-sentiment-model-indobertweet-v2',
            'agufsamudra': 'agufsamudra/indo-sentiment-analysis'  # IndoBERT-based alternative
        }

        if model_choice not in available_models:
            print(f"Model choice '{model_choice}' tidak tersedia.")
            print(f"Available options: {list(available_models.keys())}")
            model_choice = 'aardiiiiy'  # default fallback

        self.model_name = available_models[model_choice]
        print(f"Loading IndoBERTweet model: {self.model_name}")

        try:
            # Load tokenizer dan model
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)

            # Setup device
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model.to(self.device)
            self.model.eval()

            # Label mapping untuk sentiment analysis (standar 3 kelas)
            self.label_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}
            self.reverse_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}

            print(f"‚úÖ Model berhasil dimuat pada {self.device}")
            print(f"üìä Labels: {list(self.label_mapping.values())}")

        except Exception as e:
            print(f"‚ùå Error loading model {self.model_name}: {e}")
            raise e

    def preprocess_tweet(self, text):
        """
        Preprocessing khusus untuk tweet sesuai IndoBERTweet format
        """
        import re

        if pd.isna(text) or text is None:
            return ""

        text = str(text).strip()

        # Convert ke lowercase
        text = text.lower()

        # Replace user mentions dengan @USER
        text = re.sub(r'@\w+', '@USER', text)

        # Replace URLs dengan HTTPURL
        text = re.sub(r'http\S+|www\S+|https\S+', 'HTTPURL', text, flags=re.MULTILINE)

        return text

    def predict_single(self, text):
        """
        Prediksi sentimen untuk single text
        """
        try:
            # Preprocess text
            processed_text = self.preprocess_tweet(text)

            # Tokenize
            inputs = self.tokenizer(
                processed_text,
                return_tensors='pt',
                truncation=True,
                max_length=128,
                padding=True,
                add_special_tokens=True
            )

            # Move to device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Predict
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)

            # Extract results
            predicted_label_id = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][predicted_label_id].item()
            sentiment = self.label_mapping[predicted_label_id]

            # Get all probabilities
            all_probs = probabilities[0].cpu().numpy()

            return {
                'sentiment': sentiment,
                'confidence': confidence,
                'probabilities': {
                    'negative': float(all_probs[0]),
                    'neutral': float(all_probs[1]),
                    'positive': float(all_probs[2])
                }
            }

        except Exception as e:
            print(f"Error predicting text: {text[:50]}... | Error: {e}")
            return {
                'sentiment': 'neutral',
                'confidence': 0.33,
                'probabilities': {'negative': 0.33, 'neutral': 0.34, 'positive': 0.33}
            }

    def predict_batch(self, texts, batch_size=16):
        """
        Prediksi sentimen untuk batch texts dengan progress bar
        """
        results = []

        print(f"üîÑ Processing {len(texts)} texts...")

        # Process with progress bar
        for i in tqdm(range(0, len(texts), batch_size), desc="Predicting sentiment"):
            batch_texts = texts[i:i+batch_size]

            for text in batch_texts:
                result = self.predict_single(text)
                results.append(result)

        return results

# Setup model dengan error handling
def setup_indobertweet_model():
    """
    Setup IndoBERTweet model dengan multiple fallback options
    """

    model_priorities = ['aardiiiiy', 'ridhodaffasyah', 'rikidharmawan', 'agufsamudra']

    for model_choice in model_priorities:
        try:
            print(f"üîÑ Mencoba model: {model_choice}...")
            analyzer = IndoBERTweetSentimentAnalyzer(model_choice)
            print(f"‚úÖ Berhasil setup model: {model_choice}")
            return analyzer

        except Exception as e:
            print(f"‚ùå Gagal setup model {model_choice}: {e}")
            continue

    raise Exception("‚ùå Tidak ada model IndoBERTweet yang berhasil dimuat!")

# Inisialisasi model
print("=== SETUP INDOBERTWEET MODEL ===")
try:
    bert_analyzer = setup_indobertweet_model()
    print("üéâ IndoBERTweet model siap digunakan!")
except Exception as e:
    print(f"‚ùå Setup model failure: {e}")

"""INTEGRATING KEYWORD AND BERT PREDICTIONS"""

def integrate_keyword_and_bert_predictions(df, sentiment_analyzer):
    """
    Integrasikan hasil keyword-based dengan IndoBERTweet predictions
    """

    print("=== INTEGRATING KEYWORD AND BERT PREDICTIONS ===\n")

    # Ambil teks yang sudah dibersihkan
    texts = df['text_normalized'].tolist() # Use text_normalized as cleaned text

    # Prediksi menggunakan IndoBERTweet
    print("Running IndoBERTweet predictions...")
    bert_results = sentiment_analyzer.predict_batch(texts, batch_size=16)

    # Extract results ke columns
    df['bert_sentiment'] = [r['sentiment'] for r in bert_results]
    df['bert_confidence'] = [r['confidence'] for r in bert_results]
    df['bert_prob_negative'] = [r['probabilities']['negative'] for r in bert_results]
    df['bert_prob_neutral'] = [r['probabilities']['neutral'] for r in bert_results]
    df['bert_prob_positive'] = [r['probabilities']['positive'] for r in bert_results]

    return df

# Apply BERT predictions
# Use bert_analyzer which was defined in the previous cell
df_with_bert = integrate_keyword_and_bert_predictions(df, bert_analyzer)

print("BERT predictions completed!")
print(f"Data shape: {df_with_bert.shape}")

"""COMPARISON: KEYWORD vs BERT PREDICTIONS"""

def compare_predictions(df):
    """
    Bandingkan hasil keyword-based vs BERT predictions
    """

    print("=== COMPARISON: KEYWORD vs BERT PREDICTIONS ===\n")

    # Hitung agreement antara keyword dan BERT
    agreement = (df['predicted_sentiment'] == df['bert_sentiment']).mean()
    print(f"Agreement between Keyword and BERT: {agreement:.3f} ({agreement*100:.1f}%)")

    # Confusion matrix style comparison
    comparison = pd.crosstab(
        df['predicted_sentiment'],
        df['bert_sentiment'],
        margins=True
    )
    print("\nKeyword vs BERT Crosstab:")
    print(comparison)

    # Distribusi confidence BERT
    print(f"\nBERT Confidence Statistics:")
    print(df['bert_confidence'].describe())

    return comparison

def create_ensemble_prediction(df, keyword_weight=0.3, bert_weight=0.7, min_confidence=0.6):
    """
    Buat ensemble prediction dari keyword dan BERT
    """

    def ensemble_logic(row):
        keyword_pred = row['predicted_sentiment']
        bert_pred = row['bert_sentiment']
        bert_conf = row['bert_confidence']

        # Jika BERT confidence tinggi, gunakan BERT
        if bert_conf >= min_confidence:
            return bert_pred, bert_conf, 'bert_high_conf'

        # Jika keduanya agree, gunakan prediction tersebut
        elif keyword_pred == bert_pred:
            return bert_pred, bert_conf, 'agreement'

        # Jika disagreement dan BERT confidence rendah, prioritaskan keyword
        else:
            # Tapi tetap cek apakah BERT masih reasonable
            if bert_conf >= 0.4:
                return bert_pred, bert_conf, 'bert_medium_conf'
            else:
                return keyword_pred, bert_conf, 'keyword_fallback'

    # Apply ensemble logic
    ensemble_results = df.apply(ensemble_logic, axis=1)

    df['ensemble_sentiment'] = [r[0] for r in ensemble_results]
    df['ensemble_confidence'] = [r[1] for r in ensemble_results]
    df['ensemble_method'] = [r[2] for r in ensemble_results]

    # Statistics
    print("=== ENSEMBLE RESULTS ===")
    print(f"Method distribution:")
    print(df['ensemble_method'].value_counts())

    print(f"\nEnsemble sentiment distribution:")
    print(df['ensemble_sentiment'].value_counts())

    return df

# Apply comparison and ensemble
comparison_matrix = compare_predictions(df_with_bert)
df_ensemble = create_ensemble_prediction(df_with_bert)

"""FILTERING HIGH QUALITY PREDICTIONS"""

def filter_high_quality_predictions(df, min_bert_confidence=0.7, agreement_required=False):
    """
    Filter data dengan kualitas prediksi tinggi
    """

    print("=== FILTERING HIGH QUALITY PREDICTIONS ===\n")

    # Filter berdasarkan BERT confidence
    high_conf_mask = df['bert_confidence'] >= min_bert_confidence

    if agreement_required:
        # Juga butuh agreement antara keyword dan BERT
        agreement_mask = df['predicted_sentiment'] == df['bert_sentiment']
        final_mask = high_conf_mask & agreement_mask
        filter_desc = f"BERT confidence >= {min_bert_confidence} AND agreement"
    else:
        final_mask = high_conf_mask
        filter_desc = f"BERT confidence >= {min_bert_confidence}"

    # Apply filter
    df_high_quality = df[final_mask].copy()

    print(f"Filter criteria: {filter_desc}")
    print(f"Original data: {len(df)} tweets")
    print(f"High quality: {len(df_high_quality)} tweets ({len(df_high_quality)/len(df)*100:.1f}%)")

    # Distribution per sentiment
    print(f"\nHigh quality distribution:")
    print(df_high_quality['bert_sentiment'].value_counts())

    return df_high_quality

# Apply high quality filtering
df_high_quality = filter_high_quality_predictions(df_ensemble, min_bert_confidence=0.75)

def prepare_final_dataset(df_high_quality, df_ensemble):
    """
    Persiapkan dataset final dengan berbagai kategori confidence
    """

    # Kategorikan berdasarkan confidence level
    def categorize_confidence(confidence):
        if confidence >= 0.9:
            return 'very_high'
        elif confidence >= 0.7:
            return 'high'
        elif confidence >= 0.5:
            return 'medium'
        else:
            return 'low'

    df_ensemble['confidence_category'] = df_ensemble['bert_confidence'].apply(categorize_confidence)

    # Create different quality tiers
    datasets = {
        'premium': df_ensemble[df_ensemble['confidence_category'] == 'very_high'],
        'high_quality': df_ensemble[df_ensemble['confidence_category'] == 'high'],
        'medium_quality': df_ensemble[df_ensemble['confidence_category'] == 'medium'],
        'all_predictions': df_ensemble
    }

    # Statistics untuk setiap tier
    print("=== FINAL DATASET TIERS ===\n")
    for tier, data in datasets.items():
        print(f"{tier.upper()}:")
        print(f"  Total: {len(data)} tweets")
        if len(data) > 0:
            print(f"  Sentiment distribution:")
            sentiment_dist = data['ensemble_sentiment'].value_counts()
            for sentiment, count in sentiment_dist.items():
                percentage = count/len(data)*100
                print(f"    {sentiment}: {count} ({percentage:.1f}%)")
        print()

    # Export setiap tier
    for tier, data in datasets.items():
        filename = f'tweets_{tier}_indotext.csv'

        # Select relevant columns
        export_columns = [
            'tweet_text', 'cleaned_text',
            'predicted_sentiment', 'bert_sentiment', 'ensemble_sentiment',
            'bert_confidence', 'confidence_category',
            'bert_prob_negative', 'bert_prob_neutral', 'bert_prob_positive'
        ]

        available_columns = [col for col in export_columns if col in data.columns]
        data[available_columns].to_csv(filename, index=False)
        print(f"Exported: {filename}")

    return datasets

# Prepare and export final datasets
final_datasets = prepare_final_dataset(df_high_quality, df_ensemble)

def evaluate_model_performance(df):
    """
    Evaluasi performa model IndoBERTweet
    """

    print("=== MODEL PERFORMANCE EVALUATION ===\n")

    # Confidence statistics per sentiment
    print("Confidence statistics per sentiment:")
    conf_by_sentiment = df.groupby('bert_sentiment')['bert_confidence'].agg([
        'count', 'mean', 'std', 'min', 'max'
    ]).round(3)
    print(conf_by_sentiment)

    # Agreement analysis
    if 'predicted_sentiment' in df.columns:
        agreement_by_sentiment = df.groupby('bert_sentiment').apply(
            lambda x: (x['predicted_sentiment'] == x['bert_sentiment']).mean()
        ).round(3)

        print(f"\nAgreement with keyword-based method:")
        for sentiment, agreement in agreement_by_sentiment.items():
            print(f"  {sentiment}: {agreement:.3f} ({agreement*100:.1f}%)")

    # Probability distribution analysis
    print(f"\nProbability distribution analysis:")
    prob_stats = df[['bert_prob_negative', 'bert_prob_neutral', 'bert_prob_positive']].describe()
    print(prob_stats.round(3))

    return conf_by_sentiment

# Run evaluation
performance_stats = evaluate_model_performance(df_ensemble)

print("\n=== SETUP INDOBERTWEET COMPLETED ===")
print(f"‚úÖ Model loaded and ready")
print(f"‚úÖ Predictions completed for {len(df_ensemble)} tweets")
print(f"‚úÖ High quality data: {len(df_high_quality)} tweets")
print(f"‚úÖ Multiple dataset tiers exported")
print(f"‚úÖ Performance evaluation completed")

"""# SET UP PLOT HASIL SENTIMEN

Word Cloud
"""

# Install required packages untuk visualisasi
!pip install wordcloud
!pip install matplotlib
!pip install seaborn
!pip install plotly
!pip install pillow

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import plotly.graph_objects as go
import plotly.express as px
from collections import Counter
import re
from typing import List, Dict

# Set style untuk matplotlib
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Konfigurasi untuk tampilan yang lebih baik
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

def create_word_cloud(text_data, sentiment_filter=None, title="Word Cloud",
                     width=800, height=400, max_words=100):
    """
    Membuat word cloud dari data teks
    """

    # Filter berdasarkan sentiment jika diperlukan
    if sentiment_filter:
        if isinstance(sentiment_filter, str):
            filtered_data = text_data[text_data.index.isin(
                df_ensemble[df_ensemble['ensemble_sentiment'] == sentiment_filter].index
            )]
        else:
            filtered_data = text_data[text_data.index.isin(
                df_ensemble[df_ensemble['ensemble_sentiment'].isin(sentiment_filter)].index
            )]
    else:
        filtered_data = text_data

    # Gabungkan semua teks
    text = ' '.join(filtered_data.dropna().astype(str).tolist())

    # Bersihkan teks dari stopwords Indonesia
    stopwords_id = {
        'yang', 'dan', 'di', 'ke', 'dari', 'pada', 'untuk', 'dengan', 'adalah',
        'ini', 'itu', 'atau', 'jika', 'akan', 'telah', 'sudah', 'belum',
        'tidak', 'bukan', 'ada', 'juga', 'saya', 'aku', 'kamu', 'dia',
        'mereka', 'kita', 'kami', 'nya', 'an', 'kan', 'lah', 'pun'
    }

    # Konfigurasi WordCloud
    wordcloud = WordCloud(
        width=width,
        height=height,
        background_color='white',
        max_words=max_words,
        stopwords=stopwords_id,
        colormap='viridis',
        relative_scaling=0.5,
        min_font_size=10
    ).generate(text)

    # Plot
    plt.figure(figsize=(15, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=18, fontweight='bold', pad=20)
    plt.tight_layout(pad=0)
    plt.show()

    return wordcloud

# Fungsi untuk word cloud per sentimen
def create_sentiment_wordclouds(df, text_column='text_clean', sentiment_column='ensemble_sentiment'):
    """
    Membuat word cloud untuk setiap kategori sentimen
    """

    sentiments = df[sentiment_column].unique()

    # Setup subplot
    fig, axes = plt.subplots(1, len(sentiments), figsize=(20, 6))
    if len(sentiments) == 1:
        axes = [axes]

    colors = {'positive': 'Greens', 'negative': 'Reds', 'neutral': 'Blues'}

    for i, sentiment in enumerate(sentiments):
        # Filter data berdasarkan sentimen
        sentiment_data = df[df[sentiment_column] == sentiment]
        text = ' '.join(sentiment_data[text_column].dropna().astype(str).tolist())

        if text.strip():  # Jika ada teks
            stopwords_id = {
                'yang', 'dan', 'di', 'ke', 'dari', 'pada', 'untuk', 'dengan', 'adalah',
                'ini', 'itu', 'atau', 'jika', 'akan', 'telah', 'sudah', 'belum',
                'tidak', 'bukan', 'ada', 'juga', 'saya', 'aku', 'kamu', 'dia'
            }

            wordcloud = WordCloud(
                width=400, height=400,
                background_color='white',
                max_words=50,
                stopwords=stopwords_id,
                colormap=colors.get(sentiment, 'viridis')
            ).generate(text)

            axes[i].imshow(wordcloud, interpolation='bilinear')
            axes[i].axis('off')
            axes[i].set_title(f'{sentiment.upper()}\n({len(sentiment_data)} tweets)',
                            fontsize=14, fontweight='bold')
        else:
            axes[i].text(0.5, 0.5, f'No data for\n{sentiment}',
                        ha='center', va='center', transform=axes[i].transAxes)
            axes[i].axis('off')

    plt.suptitle('Word Clouds per Sentiment Category', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

# Implementasi word clouds
print("=== GENERATING WORD CLOUDS ===")

# Word cloud keseluruhan
create_word_cloud(df_ensemble['text_clean'], title="Word Cloud - All Tweets")

# Word cloud per sentimen
create_sentiment_wordclouds(df_ensemble)

"""Bar Plot Most Frequen Word"""

def get_word_frequency(text_series, top_n=20, min_length=2):
    """
    Mendapatkan frekuensi kata dari series teks
    """

    # Gabungkan semua teks
    all_text = ' '.join(text_series.dropna().astype(str).tolist()).lower()

    # Extract words (hanya huruf)
    words = re.findall(r'\b[a-zA-Z]+\b', all_text)

    # Filter berdasarkan panjang minimum
    words = [word for word in words if len(word) >= min_length]

    # Hapus stopwords
    stopwords_id = {
        'yang', 'dan', 'di', 'ke', 'dari', 'pada', 'untuk', 'dengan', 'adalah',
        'ini', 'itu', 'atau', 'jika', 'akan', 'telah', 'sudah', 'belum',
        'tidak', 'bukan', 'ada', 'juga', 'saya', 'aku', 'kamu', 'dia',
        'mereka', 'kita', 'kami', 'nya', 'kan', 'lah', 'pun', 'an'
    }

    filtered_words = [word for word in words if word not in stopwords_id]

    # Hitung frekuensi
    word_freq = Counter(filtered_words)

    return word_freq.most_common(top_n)

def plot_word_frequency_bar(word_freq, title="Top Words Frequency", color_palette="viridis"):
    """
    Membuat bar chart frekuensi kata
    """

    words, counts = zip(*word_freq)

    # Create DataFrame for easier plotting
    df_words = pd.DataFrame({'words': words, 'frequency': counts})

    # Plot horizontal bar chart
    plt.figure(figsize=(12, 8))
    bars = plt.barh(range(len(words)), counts, color=sns.color_palette(color_palette, len(words)))

    # Customization
    plt.yticks(range(len(words)), words)
    plt.xlabel('Frequency', fontsize=12, fontweight='bold')
    plt.ylabel('Words', fontsize=12, fontweight='bold')
    plt.title(title, fontsize=16, fontweight='bold', pad=20)

    # Add value labels on bars
    for i, (bar, count) in enumerate(zip(bars, counts)):
        plt.text(count + max(counts)*0.01, i, str(count),
                va='center', fontweight='bold')

    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.show()

    return df_words

def plot_sentiment_word_frequency(df, text_column='text_clean',
                                sentiment_column='ensemble_sentiment', top_n=15):
    """
    Membuat bar chart frekuensi kata per sentimen
    """

    sentiments = df[sentiment_column].unique()

    fig, axes = plt.subplots(len(sentiments), 1, figsize=(14, 6*len(sentiments)))
    if len(sentiments) == 1:
        axes = [axes]

    colors = {'positive': 'green', 'negative': 'red', 'neutral': 'blue'}

    for i, sentiment in enumerate(sentiments):
        # Filter data berdasarkan sentimen
        sentiment_data = df[df[sentiment_column] == sentiment]

        # Get word frequency
        word_freq = get_word_frequency(sentiment_data[text_column], top_n)

        if word_freq:
            words, counts = zip(*word_freq)

            # Plot
            bars = axes[i].barh(range(len(words)), counts,
                              color=colors.get(sentiment, 'gray'), alpha=0.7)

            axes[i].set_yticks(range(len(words)))
            axes[i].set_yticklabels(words)
            axes[i].set_xlabel('Frequency')
            axes[i].set_title(f'Top {top_n} Words - {sentiment.upper()} Sentiment\n({len(sentiment_data)} tweets)')
            axes[i].grid(axis='x', alpha=0.3)

            # Add value labels
            for j, (bar, count) in enumerate(zip(bars, counts)):
                axes[i].text(count + max(counts)*0.01, j, str(count),
                           va='center', fontweight='bold')
        else:
            axes[i].text(0.5, 0.5, f'No data for {sentiment}',
                        ha='center', va='center', transform=axes[i].transAxes)

    plt.suptitle('Word Frequency by Sentiment', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

# Implementasi bar charts
print("=== GENERATING WORD FREQUENCY BAR CHARTS ===")

# Overall word frequency
overall_word_freq = get_word_frequency(df_ensemble['text_clean'], top_n=25)
df_word_freq = plot_word_frequency_bar(overall_word_freq,
                                     title="Top 25 Most Frequent Words - All Tweets")

# Word frequency per sentiment
plot_sentiment_word_frequency(df_ensemble, top_n=15)

"""Pie Chart Distribusi Sentimen"""

def plot_sentiment_pie_chart(df, sentiment_column='ensemble_sentiment',
                           title="Distribusi Sentimen", figsize=(10, 8)):
    """
    Membuat pie chart distribusi sentimen
    """

    # Hitung distribusi
    sentiment_counts = df[sentiment_column].value_counts()

    # Warna untuk setiap sentimen
    colors = {
        'positive': '#2E8B57',  # Sea Green
        'negative': '#DC143C',  # Crimson
        'neutral': '#4682B4'    # Steel Blue
    }

    # Map colors based on sentiment
    pie_colors = [colors.get(sentiment, '#808080') for sentiment in sentiment_counts.index]

    # Create pie chart
    plt.figure(figsize=figsize)
    wedges, texts, autotexts = plt.pie(
        sentiment_counts.values,
        labels=sentiment_counts.index,
        autopct=lambda pct: f'{pct:.1f}%\n({int(pct/100*len(df))})',
        startangle=90,
        colors=pie_colors,
        explode=[0.05 if sentiment == 'positive' else 0 for sentiment in sentiment_counts.index],
        shadow=True
    )

    # Customization
    plt.title(title, fontsize=16, fontweight='bold', pad=20)

    # Beautify text
    for text in texts:
        text.set_fontsize(12)
        text.set_fontweight('bold')

    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(10)

    plt.axis('equal')

    # Add legend with counts
    legend_labels = [f'{sentiment}: {count}' for sentiment, count in sentiment_counts.items()]
    plt.legend(wedges, legend_labels, title="Sentiments", loc="center left",
              bbox_to_anchor=(1, 0, 0.5, 1))

    plt.tight_layout()
    plt.show()

    return sentiment_counts

def plot_confidence_distribution(df, confidence_column='bert_confidence'):
    """
    Membuat distribusi confidence levels
    """

    # Kategorikan confidence
    def categorize_confidence(conf):
        if conf >= 0.9:
            return 'Very High (‚â•90%)'
        elif conf >= 0.7:
            return 'High (70-89%)'
        elif conf >= 0.5:
            return 'Medium (50-69%)'
        else:
            return 'Low (<50%)'

    df['confidence_category'] = df[confidence_column].apply(categorize_confidence)
    conf_counts = df['confidence_category'].value_counts()

    # Plot pie chart for confidence distribution
    plt.figure(figsize=(10, 8))
    colors = ['#228B22', '#32CD32', '#FFD700', '#FF6347']

    wedges, texts, autotexts = plt.pie(
        conf_counts.values,
        labels=conf_counts.index,
        autopct=lambda pct: f'{pct:.1f}%\n({int(pct/100*len(df))})',
        startangle=90,
        colors=colors,
        shadow=True
    )

    plt.title('Distribution of Prediction Confidence Levels',
              fontsize=16, fontweight='bold', pad=20)

    for text in texts:
        text.set_fontsize(11)
        text.set_fontweight('bold')

    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')

    plt.axis('equal')
    plt.tight_layout()
    plt.show()

    return conf_counts

def create_comprehensive_sentiment_plots(df):
    """
    Membuat plot komprehensif untuk analisis sentimen
    """

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))

    # 1. Sentiment Distribution Pie Chart
    sentiment_counts = df['ensemble_sentiment'].value_counts()
    colors = ['#2E8B57', '#DC143C', '#4682B4']
    ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',
            startangle=90, colors=colors, shadow=True)
    ax1.set_title('Sentiment Distribution', fontweight='bold')

    # 2. Confidence Distribution Histogram
    ax2.hist(df['bert_confidence'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    ax2.set_xlabel('BERT Confidence Score')
    ax2.set_ylabel('Frequency')
    ax2.set_title('Distribution of Confidence Scores', fontweight='bold')
    ax2.grid(True, alpha=0.3)

    # 3. Sentiment vs Confidence Box Plot
    sentiment_order = ['negative', 'neutral', 'positive']
    available_sentiments = [s for s in sentiment_order if s in df['ensemble_sentiment'].unique()]

    box_data = [df[df['ensemble_sentiment'] == sentiment]['bert_confidence']
                for sentiment in available_sentiments]
    box_colors = ['red', 'gray', 'green'][:len(available_sentiments)]

    bp = ax3.boxplot(box_data, labels=available_sentiments, patch_artist=True)
    for patch, color in zip(bp['boxes'], box_colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax3.set_xlabel('Sentiment')
    ax3.set_ylabel('BERT Confidence')
    ax3.set_title('Confidence Score by Sentiment', fontweight='bold')
    ax3.grid(True, alpha=0.3)

    # 4. Top 10 Words Bar Chart
    word_freq = get_word_frequency(df['text_clean'], top_n=10)
    if word_freq:
        words, counts = zip(*word_freq)
        bars = ax4.barh(range(len(words)), counts, color='lightcoral')
        ax4.set_yticks(range(len(words)))
        ax4.set_yticklabels(words)
        ax4.set_xlabel('Frequency')
        ax4.set_title('Top 10 Most Frequent Words', fontweight='bold')

        # Add value labels
        for i, (bar, count) in enumerate(zip(bars, counts)):
            ax4.text(count + max(counts)*0.01, i, str(count), va='center')

    plt.tight_layout()
    plt.show()

# Implementasi pie charts dan analisis komprehensif
print("=== GENERATING SENTIMENT DISTRIBUTION CHARTS ===")

# Pie chart sentimen utama
sentiment_dist = plot_sentiment_pie_chart(df_ensemble,
                                        title="Distribusi Sentimen - IndoBERTweet Analysis")

# Pie chart confidence levels
confidence_dist = plot_confidence_distribution(df_ensemble)

# Comprehensive plots
create_comprehensive_sentiment_plots(df_ensemble)

"""Premium dataset chart

"""

import matplotlib.pyplot as plt
import pandas as pd

def create_premium_pie_chart_from_data():
    """
    Membuat pie chart dari data PREMIUM dataset yang sudah diketahui
    """

    # Data dari PREMIUM dataset
    premium_data = {
        'negative': 86,
        'neutral': 80,
        'positive': 29
    }

    total_tweets = sum(premium_data.values())  # 195 tweets

    print("="*60)
    print("üèÜ PREMIUM DATASET SENTIMENT ANALYSIS")
    print("="*60)
    print(f"üìä Total Premium Tweets: {total_tweets}")
    print(f"üìà Confidence Threshold: ‚â•90%")
    print("\nüìã DISTRIBUSI SENTIMEN:")
    print("-" * 40)

    # Print statistik detail
    for sentiment, count in premium_data.items():
        percentage = (count / total_tweets) * 100
        print(f"  {sentiment.capitalize():>8}: {count:>3} tweets ({percentage:>5.1f}%)")

    # Setup pie chart
    labels = list(premium_data.keys())
    sizes = list(premium_data.values())
    colors = ['#DC143C', '#4682B4', '#2E8B57']  # Crimson, Steel Blue, Sea Green
    explode = (0.05, 0.05, 0.1)  # Explode positive slice sedikit lebih besar

    # Create pie chart
    plt.figure(figsize=(12, 10))

    wedges, texts, autotexts = plt.pie(
        sizes,
        labels=[label.capitalize() for label in labels],
        autopct=lambda pct: f'{pct:.1f}%\n({int(pct/100*total_tweets)} tweets)',
        startangle=90,
        colors=colors,
        explode=explode,
        shadow=True,
        textprops={'fontsize': 12, 'fontweight': 'bold'}
    )

    # Customize chart
    plt.title('Distribusi Sentimen Dataset PREMIUM\n(High-Confidence Predictions ‚â•90%)',
              fontsize=16, fontweight='bold', pad=20)

    # Beautify text
    for text in texts:
        text.set_fontsize(14)
        text.set_fontweight('bold')
        text.set_color('darkblue')

    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(11)

    # Add dataset info
    plt.figtext(0.5, 0.02, f'Premium Dataset: {total_tweets} tweets with confidence ‚â•90%',
                ha='center', fontsize=10, style='italic', color='gray')

    plt.axis('equal')
    plt.tight_layout()
    plt.show()

    # Print analisis detail
    print("\n" + "="*60)
    print("üìà DETAILED ANALYSIS")
    print("="*60)

    # Sentiment dominan
    dominant_sentiment = max(premium_data, key=premium_data.get)
    print(f"üéØ Dominant Sentiment: {dominant_sentiment.upper()}")
    print(f"   ‚Ä¢ Count: {premium_data[dominant_sentiment]} tweets")
    print(f"   ‚Ä¢ Percentage: {premium_data[dominant_sentiment]/total_tweets*100:.1f}%")

    # Analisis sentiment balance
    pos_count = premium_data['positive']
    neg_count = premium_data['negative']
    neu_count = premium_data['neutral']

    print(f"\nüîç SENTIMENT CHARACTERISTICS:")
    print(f"   ‚Ä¢ Negative-leaning dataset (44.1% negative)")
    print(f"   ‚Ä¢ High neutral content (41.0% neutral)")
    print(f"   ‚Ä¢ Low positive sentiment (14.9% positive)")
    print(f"   ‚Ä¢ Negative:Positive ratio = {neg_count/pos_count:.1f}:1")

    print(f"\nüìä DISTRIBUTION INSIGHTS:")
    print(f"   ‚Ä¢ Combined Negative+Neutral: {(neg_count+neu_count)/total_tweets*100:.1f}%")
    print(f"   ‚Ä¢ Sentiment Polarization: Moderate (high neutral)")
    print(f"   ‚Ä¢ Dataset Tone: PREDOMINANTLY NEGATIVE")

    print("\n" + "="*60)
    print("‚úÖ PREMIUM PIE CHART ANALYSIS COMPLETED")
    print("="*60)

    return premium_data

# JALANKAN ANALISIS LANGSUNG
print("üöÄ Creating Premium Dataset Pie Chart...")
results = create_premium_pie_chart_from_data()