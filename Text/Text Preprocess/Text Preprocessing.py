# -*- coding: utf-8 -*-
"""MBD_D_Kelompok1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p0-cSWHlEzmQ3Hs70pDxQCqW5KVZwdAz

# Load Library
"""

!pip install nltk spacy emoji regex
!python -m spacy download en_core_web_sm

import re
import os
import zipfile
import glob
from typing import List
import pandas as pd
import nltk
import emoji
import string

# NLTK assets
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import spacy
nlp = spacy.load("en_core_web_sm")  # untuk lemmatization / POS-aware (Data Berita)

URL_PATTERN = re.compile(
    r'http[s]?://\S+|www\.\S+'
)

MENTION_PATTERN = re.compile(r'@\w+')
HASHTAG_PATTERN = re.compile(r'#\w+')
# Updated to create emoji pattern from emoji.EMOJI_DATA.keys()
EMOJI_PATTERN = re.compile(
    '|'.join(re.escape(e) for e in emoji.EMOJI_DATA.keys())
)
MULTI_SPACES = re.compile(r'\s+')

PUNCT_TABLE = str.maketrans('', '', string.punctuation)

"""# Import & Fungsi"""

import os, glob, zipfile
import pandas as pd
import re, string
import emoji

# ========= FUNGSI BACA TEKS =========
def read_texts(path: str, ext: str = ".txt", prefix: str = None) -> pd.DataFrame:
    rows = []

    if os.path.isdir(path):  # Folder
        files = sorted(glob.glob(os.path.join(path, f"*{ext}")))
        for file in files:
            with open(file, "r", encoding="utf-8", errors="ignore") as f:
                rows.append({"id": os.path.basename(file), "text": f.read()})
        return pd.DataFrame(rows)

    elif zipfile.is_zipfile(path):  # ZIP
        with zipfile.ZipFile(path, 'r') as z:
            for name in sorted(z.namelist()):
                if not name.lower().endswith(ext):
                    continue
                if prefix and not name.startswith(prefix):
                    continue
                with z.open(name) as f:
                    text = f.read().decode("utf-8", errors="ignore")
                rows.append({"id": os.path.basename(name), "text": text})
        return pd.DataFrame(rows)

    else:
        raise ValueError("Path bukan folder atau ZIP")

# ========= PREPROCESSING MINIMAL (RECOMMENDED) =========
URL_PATTERN = re.compile(r'http[s]?://\S+|www\.\S+')
MENTION_PATTERN = re.compile(r'@\w+')
HASHTAG_PATTERN = re.compile(r'#(\w+)')
MULTI_SPACES = re.compile(r'\s+')
EMOJI_PATTERN = re.compile(
    '|'.join(re.escape(e) for e in emoji.EMOJI_DATA.keys())
)

def basic_preprocess(text):
    t = str(text)

    t = t.lower()
    t = URL_PATTERN.sub('', t)
    t = MENTION_PATTERN.sub('', t)
    t = HASHTAG_PATTERN.sub(r'\1', t)
    t = t.replace('\n', ' ').replace('\r', ' ')
    t = MULTI_SPACES.sub(' ', t).strip()

    return t

# ========= TOKENIZATION SEDERHANA =========
def tokenize_text(text):
    return re.findall(r"\w+|[^\w\s]", text)

def apply_tweet_pipeline(df, text_col='text', out_col='tokens'):
    df[out_col] = df[text_col].apply(lambda x: tokenize_text(basic_preprocess(x)))
    return df

"""# Data Tweets

## Lowercasing (1)
"""

def lowercasing(text: str) -> str:
    if text is None: return text
    return text.lower()

"""## Hapus URLs"""

def remove_urls(text: str) -> str:
    return URL_PATTERN.sub('', text)

"""## Hapus Tag"""

def remove_hashtags(text: str, keep_tag_text: bool=False) -> str:
    # keep_tag_text=False: hapus seluruh '#tag'
    # keep_tag_text=True: ubah '#happy' -> 'happy'
    if keep_tag_text:
        return re.sub(r'#(\w+)', r'\1', text)
    return HASHTAG_PATTERN.sub('', text)

def remove_usernames(text: str) -> str:
    return MENTION_PATTERN.sub('', text)

"""## Hapus Emoji"""

def remove_emojis(text: str) -> str:
    # emoji lib handles most emoji
    return EMOJI_PATTERN.sub('', text)

"""## Hapus Tanda Baca, Angka, atau Simpan Angka (2)"""

def remove_punct_and_digits(text: str, keep_digits: bool=False) -> str:
    if keep_digits:
        # hanya hapus tanda baca
        return text.translate(str.maketrans('', '', string.punctuation))
    else:
        return re.sub(r'[\d' + re.escape(string.punctuation) + ']+', ' ', text)

"""## Hapus Enter / normalize space"""

def remove_enter_and_normalize(text: str) -> str:
    t = text.replace('\r', ' ').replace('\n', ' ')
    t = MULTI_SPACES.sub(' ', t).strip()
    return t

"""## Tokenisasi (3)"""

def tokenize_nltk(text: str) -> List[str]:
    return word_tokenize(text)

# contoh tokenisasi 'tweet-aware' sederhana:
def tokenize_tweet(text: str) -> List[str]:
    # memecah berdasarkan spasi lalu pisah punctuation minimal
    tokens = re.findall(r"\w+|[^\w\s]", text, re.UNICODE)
    return tokens

"""## Hapus Stop Word (4)"""

STOPWORDS_EN = set(stopwords.words('english'))

def remove_stopwords(tokens: List[str], extra_stopwords: List[str]=None) -> List[str]:
    s = STOPWORDS_EN.copy()
    if extra_stopwords:
        s |= set(extra_stopwords)
    return [t for t in tokens if t.lower() not in s and t.isalpha()]

"""## Lemmatization (5)"""

lemmatizer = WordNetLemmatizer()

def lemmatize_tokens_nltk(tokens: List[str]) -> List[str]:
    return [lemmatizer.lemmatize(t) for t in tokens]

def lemmatize_spacy(text: str, allowed_postags: List[str]=["NOUN","ADJ","VERB","ADV"]) -> str:
    doc = nlp(text)
    lemmas = [token.lemma_ for token in doc if token.pos_ in allowed_postags]
    return " ".join(lemmas)

"""# Terapkan ke DataFrame"""

def preprocess_tweet(text: str) -> List[str]:
    t = str(text)
    t = lowercasing(t)
    t = remove_urls(t)
    t = remove_usernames(t)
    t = remove_emojis(t)
    t = remove_hashtags(t, keep_tag_text=True) # Keep tag text
    t = remove_punct_and_digits(t, keep_digits=False) # Remove punct and digits
    t = remove_enter_and_normalize(t)
    tokens = tokenize_tweet(t)
    tokens = remove_stopwords(tokens) # Remove stopwords
    tokens = lemmatize_tokens_nltk(tokens) # Lemmatize
    return tokens

def preprocess_news(text: str) -> List[str]:
    t = str(text)
    t = lowercasing(t)
    t = remove_urls(t)
    t = remove_usernames(t)
    t = remove_emojis(t)
    t = remove_hashtags(t, keep_tag_text=True)
    t = remove_punct_and_digits(t, keep_digits=False)
    t = remove_enter_and_normalize(t)
    t = lemmatize_spacy(t) # SpaCy lemmatization returns a string
    tokens = tokenize_nltk(t) # Then tokenize the lemmatized string
    return tokens

# contoh: df memiliki kolom 'text'
def apply_tweet_pipeline(df: pd.DataFrame, text_col: str='text', out_col: str='tweet_tokens'):
    df[out_col] = df[text_col].fillna('').apply(lambda x: preprocess_tweet(x))
    return df

def apply_news_pipeline(df: pd.DataFrame, text_col: str='text', out_col: str='news_tokens'):
    df[out_col] = df[text_col].fillna('').apply(lambda x: preprocess_news(x))
    return df

"""# Hasil"""

# 1) baca dari zip
df = read_texts('/content/Text Pororo.zip', prefix='texts/')
print(df.shape)
print(df.head())

# 2) preview isi
for i in range(3):
    print(f"--- {df.loc[i,'id']} ---")
    print(df.loc[i,'text'][:400])

# 3) terapkan pipeline tweet-like
df = apply_tweet_pipeline(df, text_col='text', out_col='tokens')

# 4) gabungkan tokens jadi string bersih
df['clean_text'] = df['tokens'].apply(
    lambda toks: ' '.join(toks) if isinstance(toks, list) else ''
)

# 5) simpan hasil
df[['id', 'clean_text']].to_csv('pororo_clean_text.csv',
                                index=False, encoding='utf-8')

df.head()

"""# Menampilkan Semua Data"""

pd.set_option('display.max_rows', None)
pd.set_option('display.max_colwidth', None)

df

"""# TF-IDF (6)"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(
    max_features=5000, # Batasi jumlah fitur (kata) untuk menghindari "curse of dimensionality"
    ngram_range=(1, 2) # Menggunakan unigram dan bigram
)

# Terapkan TF-IDF ke kolom 'clean_text'
X_tfidf = tfidf_vectorizer.fit_transform(df['clean_text'])

# Konversi hasil ke DataFrame untuk memudahkan inspeksi (opsional)
df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

print("Shape of TF-IDF matrix:", X_tfidf.shape)
print("First 5 rows of TF-IDF DataFrame (sample):")
print(df_tfidf.head())

"""## Analisis Hasil Vektorisasi"""

# Lihat distribusi nilai non-zero
import numpy as np
non_zero_count = np.count_nonzero(X_tfidf.toarray(), axis=1)
print(f"Rata-rata kata per dokumen: {non_zero_count.mean()}")

# Lihat kata-kata dengan bobot tertinggi di dokumen pertama
feature_names = tfidf_vectorizer.get_feature_names_out()
doc_0 = X_tfidf[0].toarray().flatten()
top_indices = doc_0.argsort()[-10:][::-1]
print("Top 10 kata penting di dokumen pertama:")
for idx in top_indices:
    if doc_0[idx] > 0:
        print(f"{feature_names[idx]}: {doc_0[idx]:.3f}")